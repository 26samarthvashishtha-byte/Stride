<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>AI Object Distance Announcer</title>

<!-- TensorFlow.js + COCO-SSD -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<style>
  body {
    margin: 0;
    background: #000;
    color: #fff;
    font-family: sans-serif;
    text-align: center;
  }
  h2 {
    margin: 0.5em 0;
  }
  video, canvas {
    position: absolute;
    top: 60px;
    left: 50%;
    transform: translateX(-50%);
    width: 95vw;
    height: auto;
    border-radius: 10px;
  }
  canvas {
    pointer-events: none;
  }
  #status {
    position: fixed;
    bottom: 10px;
    left: 50%;
    transform: translateX(-50%);
    background: rgba(0, 0, 0, 0.5);
    padding: 8px 12px;
    border-radius: 6px;
    font-size: 16px;
  }
</style>
</head>
<body>
<h2>AI Object Distance Announcer</h2>
<video id="video" autoplay playsinline muted></video>
<canvas id="canvas"></canvas>
<div id="status">Loading model...</div>

<script>
let model;
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");
const status = document.getElementById("status");

let lastSpoken = "";
let lastTime = 0;

// Speak function
function speakNow(text) {
  const now = Date.now();
  if (text === lastSpoken && now - lastTime < 2000) return;
  lastSpoken = text;
  lastTime = now;
  window.speechSynthesis.cancel();
  const utterance = new SpeechSynthesisUtterance(text);
  utterance.lang = "en-US";
  utterance.rate = 1;
  window.speechSynthesis.speak(utterance);
}

// Access camera
async function initCamera() {
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "environment" },
    audio: false,
  });
  video.srcObject = stream;
  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      video.play();
      resolve();
    };
  });
}

// Object detection loop
async function detectFrame() {
  const predictions = await model.detect(video);
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

  const spoken = [];

  for (const p of predictions) {
    if (p.score >= 0.6) {
      const [x, y, w, h] = p.bbox;
      const area = (w * h) / (canvas.width * canvas.height);
      let distanceLabel;

      // Estimate closeness by bounding box size
      if (area > 0.25) distanceLabel = "very close";
      else if (area > 0.1) distanceLabel = "close";
      else distanceLabel = "far away";

      ctx.strokeStyle = "#00FF00";
      ctx.lineWidth = 2;
      ctx.strokeRect(x, y, w, h);
      ctx.fillStyle = "#00FF00";
      ctx.font = "16px sans-serif";
      ctx.fillText(`${p.class} (${distanceLabel})`, x, y > 20 ? y - 5 : 10);

      spoken.push(`${p.class} ${distanceLabel}`);
    }
  }

  if (spoken.length) {
    const unique = [...new Set(spoken)];
    const phrase = unique.join(", ");
    status.textContent = "Detected: " + phrase;
    speakNow(phrase);
  } else {
    status.textContent = "No confident objects detected";
  }

  requestAnimationFrame(detectFrame);
}

// Main
async function main() {
  await initCamera();
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
  await tf.setBackend("webgl");
  model = await cocoSsd.load();
  status.textContent = "Model loaded! Detecting...";
  detectFrame();
}
main();
</script>
</body>
</html>
