<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Smart Object Detection with Voice</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body {
      margin: 0;
      font-family: system-ui, sans-serif;
      background: #111;
      color: #eee;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    video, canvas {
      border-radius: 12px;
      max-width: 90vw;
      height: auto;
      margin-top: 20px;
    }
    h1 {
      margin-top: 10px;
      font-size: 1.4rem;
      color: #00ff99;
    }
  </style>
</head>
<body>
  <h1>Real-Time Object Detection (with Speech)</h1>
  <video id="webcam" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>

  <script>
    let model;
    const video = document.getElementById("webcam");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");

    // --- SETTINGS ---
    const detectionInterval = 150; // ms between detections
    const minConfidence = 0.6;     // ignore lower confidence
    const allowedClasses = null;   // or like ["person", "dog", "car"]
    let lastSpoken = "";
    let speakCooldown = 0;

    // --- Load Model and Video ---
    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 1280, height: 720, facingMode: "environment" },
        audio: false
      });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => resolve(video);
      });
    }

    async function init() {
      try {
        await tf.setBackend('webgpu');  // faster GPU mode if supported
        await tf.ready();
      } catch (e) {
        console.warn("WebGPU not available, using fallback CPU/WebGL backend.");
      }
      model = await cocoSsd.load();
      await setupCamera();
      video.play();
      detectLoop();
    }

    // --- Speech Utility ---
    function speak(text) {
      if (!text || text === lastSpoken || speakCooldown > 0) return;
      window.speechSynthesis.cancel();
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 1.1;
      window.speechSynthesis.speak(utterance);
      lastSpoken = text;
      speakCooldown = 10; // ~1.5 sec cooldown
    }

    // --- Detection Loop ---
    async function detectLoop() {
      if (video.readyState === 4) {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.filter = "contrast(120%) brightness(110%)";
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        const predictions = await model.detect(video);
        let spokenObjects = [];

        for (let p of predictions) {
          if (p.score >= minConfidence && (!allowedClasses || allowedClasses.includes(p.class))) {
            const [x, y, w, h] = p.bbox;
            ctx.strokeStyle = "#00ff99";
            ctx.lineWidth = 2;
            ctx.strokeRect(x, y, w, h);
            ctx.fillStyle = "#00ff99";
            ctx.font = "16px sans-serif";
            ctx.fillText(`${p.class} (${(p.score*100).toFixed(1)}%)`, x, y > 20 ? y - 5 : y + 15);
            spokenObjects.push(p.class);
          }
        }

        if (spokenObjects.length > 0) {
          speak("I see " + spokenObjects.join(", "));
        }
      }

      if (speakCooldown > 0) speakCooldown--;
      setTimeout(detectLoop, detectionInterval);
    }

    init();
  </script>
</body>
</html>
